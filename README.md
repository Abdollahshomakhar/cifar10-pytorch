# Vision Transformer (ViT) from Scratch on CIFAR-10 üî•

This repository contains a **from-scratch PyTorch implementation of a Vision Transformer (ViT)** trained on the **CIFAR-10** dataset.  
All core components of ViT ‚Äî including **patch embedding, multi-head self-attention, transformer encoder blocks, and class token** ‚Äî are implemented manually without using high-level ViT libraries.

---

## üöÄ Project Highlights

- ‚úÖ Vision Transformer implemented **from scratch**
- ‚úÖ Custom **Patch Embedding** using Conv2D
- ‚úÖ Multi-Head Self-Attention (MHSA)
- ‚úÖ Pre-Norm Transformer Encoder Blocks
- ‚úÖ Truncated Normal weight initialization (ViT-style)
- ‚úÖ Cosine Annealing Learning Rate Scheduler
- ‚úÖ Data augmentation for CIFAR-10
- ‚úÖ Training & evaluation pipeline
- ‚úÖ Visualization of predictions vs ground truth

---

## üß† Model Architecture

- **Input Image Size:** 32√ó32 (CIFAR-10)
- **Patch Size:** 4√ó4 ‚Üí 64 patches
- **Embedding Dimension:** 192
- **Transformer Depth:** 6 blocks
- **Attention Heads:** 6
- **MLP Ratio:** 4
- **Classifier Token:** ‚úîÔ∏è
- **Positional Embedding:** Learnable

---

## üìä Training Results

| Epochs | Best Test Accuracy |
|------|--------------------|
| 5    | **56.86%** |

> ‚ö†Ô∏è Note: This accuracy is achieved with a **small ViT trained for only 5 epochs**.  
> Higher accuracy can be obtained by increasing depth, embedding size, epochs, or using longer training schedules.

---

## üñºÔ∏è Sample Prediction

The model correctly predicts the class of unseen test images:
Ground Truth: frog
Prediction: frog


