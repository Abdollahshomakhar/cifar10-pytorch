{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHkeSSEbAqjPMH+3Zak2sC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdollahshomakhar/cifar10-pytorch/blob/main/cifar10_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import math\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "OIc7Um5pVNEe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # From timm library, for initializing weights more effectively\n",
        "    # https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py\n",
        "    def norm_cdf(x):\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "        tensor.erfinv_()\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    _trunc_normal_(tensor, mean=mean, std=std, a=a, b=b)"
      ],
      "metadata": {
        "id": "MCw1T0GOVUIk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Splits image into patches and embeds them.\n",
        "    A Conv2d with kernel_size=patch_size and stride=patch_size performs this efficiently.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
        "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
        "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention (MHSA) mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, qkv_bias=False, qk_scale=None, attn_drop_rate=0., proj_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # Combined QKV linear projection\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop_rate)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape # B: batch_size, N: sequence_length, C: embed_dim\n",
        "\n",
        "        # qkv: (B, N, C*3) -> (B, N, 3, num_heads, head_dim) -> (3, B, num_heads, N, head_dim)\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # q, k, v: (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Attention: (B, num_heads, N, N)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        # Weighted sum of values: (B, num_heads, N, head_dim)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A two-layer Feed-Forward Network with GELU activation and dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features, out_features, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    One block of the Transformer Encoder.\n",
        "    Consists of LayerNorm, MHSA, Dropout, Residual Connection,\n",
        "    LayerNorm, MLP, Dropout, Residual Connection.\n",
        "    (Pre-normalization is used here, which is common in ViT for better stability)\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(\n",
        "            embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "            attn_drop_rate=attn_drop_rate, proj_drop_rate=drop_rate)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=embed_dim, hidden_features=mlp_hidden_dim,\n",
        "                       out_features=embed_dim, drop=drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-normalization and residual connections\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 3. The Vision Transformer Model\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) model for image classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True,\n",
        "                 qk_scale=None, drop_rate=0., attn_drop_rate=0., pos_drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_features = self.embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbeddings(\n",
        "            img_size=img_size, patch_size=patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Learnable Class Token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        # Learnable Positional Embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
        "\n",
        "        # Transformer Encoder Blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                embed_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
        "                qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop_rate=drop_rate, attn_drop_rate=attn_drop_rate\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Initialize weights\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embeddings\n",
        "        x = self.patch_embed(x) # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Add class token\n",
        "        cls_token = self.cls_token.expand(B, -1, -1) # (B, 1, embed_dim)\n",
        "        x = torch.cat((cls_token, x), dim=1) # (B, num_patches + 1, embed_dim)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Apply final LayerNorm\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Return the [CLS] token's output for classification\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_JDe_y2JVZ3U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 32 # CIFAR-10 images are 32x32\n",
        "PATCH_SIZE = 4 # Small patches for small images (32/4 = 8x8 patches)\n",
        "IN_CHANNELS = 3\n",
        "NUM_CLASSES = 10 # CIFAR-10 has 10 classes\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 5 # You'll need more for better performance\n",
        "EMBED_DIM = 192 # Smaller embedding dimension for smaller models\n",
        "DEPTH = 6      # Fewer transformer blocks\n",
        "NUM_HEADS = 6  # Fewer attention heads\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5muup9NhVcfe",
        "outputId": "a709dc53-05d0-4969-cdb4-5bc941c06e18"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "3hODj3wYViBL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, trainset.data.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z43tYWjrXoDU",
        "outputId": "dc4ab3a9-f6bc-4319-99ad-c5eb995e93e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset CIFAR10\n",
              "     Number of datapoints: 50000\n",
              "     Root location: ./data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                RandomCrop(size=(32, 32), padding=4)\n",
              "                RandomHorizontalFlip(p=0.5)\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
              "            ),\n",
              " (50000, 32, 32, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(testloader))\n",
        "print(images.shape, labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68n6ataYYxd0",
        "outputId": "edd7d559-d67b-49e8-aeba-7079fc75773b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32]) torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuzgPtE5Y_XD",
        "outputId": "8a98a047-42c8-4c52-c268-44d1d213afff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
              "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
              "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
              "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
              "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
              "        8, 3, 1, 2, 8, 0, 8, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(\n",
        "    img_size=IMG_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    in_channels=IN_CHANNELS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    depth=DEPTH,\n",
        "    num_heads=NUM_HEADS,\n",
        "    mlp_ratio=4.,\n",
        "    qkv_bias=True,\n",
        "    drop_rate=0.1,\n",
        "    attn_drop_rate=0.1,\n",
        "    pos_drop_rate=0.1\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "yKx6Siz5Vkpz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch: {epoch+1}/{EPOCHS} | Batch: {batch_idx}/{len(trainloader)} | \"\n",
        "                  f\"Loss: {running_loss/(batch_idx+1):.4f} | Acc: {100.*correct/total:.2f}%\")\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1} finished. Time: {end_time - start_time:.2f}s, \"\n",
        "          f\"Avg Loss: {running_loss/len(trainloader):.4f}, Train Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# --- Testing Function ---\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss/len(testloader):.4f} | Test Acc: {100.*correct/total:.2f}%\")\n",
        "    return 100.*correct/total\n",
        "\n",
        "# --- Main Training Loop ---\n",
        "best_acc = 0\n",
        "print(\"\\nStarting Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "    scheduler.step()\n",
        "    acc = test(epoch)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        # torch.save(model.state_dict(), './best_vit_cifar10.pth') # Uncomment to save model\n",
        "        print(f\"New best accuracy: {best_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining finished! Best Test Accuracy: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwVOynE_VphU",
        "outputId": "a00893f1-40e9-4dfd-a824-579a77ea2609"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training...\n",
            "Epoch: 1/5 | Batch: 0/391 | Loss: 2.3774 | Acc: 9.38%\n",
            "Epoch: 1/5 | Batch: 100/391 | Loss: 2.0192 | Acc: 23.54%\n",
            "Epoch: 1/5 | Batch: 200/391 | Loss: 1.9285 | Acc: 26.59%\n",
            "Epoch: 1/5 | Batch: 300/391 | Loss: 1.8795 | Acc: 28.32%\n",
            "Epoch 1 finished. Time: 31.77s, Avg Loss: 1.8410, Train Acc: 29.90%\n",
            "Test Loss: 1.7497 | Test Acc: 35.51%\n",
            "New best accuracy: 35.51%\n",
            "Epoch: 2/5 | Batch: 0/391 | Loss: 1.8634 | Acc: 35.16%\n",
            "Epoch: 2/5 | Batch: 100/391 | Loss: 1.6739 | Acc: 37.47%\n",
            "Epoch: 2/5 | Batch: 200/391 | Loss: 1.6399 | Acc: 38.86%\n",
            "Epoch: 2/5 | Batch: 300/391 | Loss: 1.6224 | Acc: 39.68%\n",
            "Epoch 2 finished. Time: 30.67s, Avg Loss: 1.6011, Train Acc: 40.60%\n",
            "Test Loss: 1.5289 | Test Acc: 44.89%\n",
            "New best accuracy: 44.89%\n",
            "Epoch: 3/5 | Batch: 0/391 | Loss: 1.4333 | Acc: 48.44%\n",
            "Epoch: 3/5 | Batch: 100/391 | Loss: 1.4613 | Acc: 46.34%\n",
            "Epoch: 3/5 | Batch: 200/391 | Loss: 1.4502 | Acc: 46.69%\n",
            "Epoch: 3/5 | Batch: 300/391 | Loss: 1.4442 | Acc: 47.00%\n",
            "Epoch 3 finished. Time: 30.75s, Avg Loss: 1.4309, Train Acc: 47.52%\n",
            "Test Loss: 1.3744 | Test Acc: 50.29%\n",
            "New best accuracy: 50.29%\n",
            "Epoch: 4/5 | Batch: 0/391 | Loss: 1.3221 | Acc: 50.78%\n",
            "Epoch: 4/5 | Batch: 100/391 | Loss: 1.3322 | Acc: 51.28%\n",
            "Epoch: 4/5 | Batch: 200/391 | Loss: 1.3236 | Acc: 51.71%\n",
            "Epoch: 4/5 | Batch: 300/391 | Loss: 1.3150 | Acc: 51.86%\n",
            "Epoch 4 finished. Time: 31.10s, Avg Loss: 1.3054, Train Acc: 52.14%\n",
            "Test Loss: 1.2168 | Test Acc: 55.79%\n",
            "New best accuracy: 55.79%\n",
            "Epoch: 5/5 | Batch: 0/391 | Loss: 1.0999 | Acc: 60.16%\n",
            "Epoch: 5/5 | Batch: 100/391 | Loss: 1.2351 | Acc: 55.28%\n",
            "Epoch: 5/5 | Batch: 200/391 | Loss: 1.2145 | Acc: 55.91%\n",
            "Epoch: 5/5 | Batch: 300/391 | Loss: 1.2129 | Acc: 55.92%\n",
            "Epoch 5 finished. Time: 30.60s, Avg Loss: 1.2132, Train Acc: 55.94%\n",
            "Test Loss: 1.1906 | Test Acc: 56.86%\n",
            "New best accuracy: 56.86%\n",
            "\n",
            "Training finished! Best Test Accuracy: 56.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZi2MgXLVHnO",
        "outputId": "2e805e0a-ea24-441c-bccd-c16cb66e867e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 9\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "    # Assuming 'image' is a preprocessed tensor suitable for the model (e.g., (1, 3, 32, 32))\n",
        "sample_image = torch.randn(1, IN_CHANNELS, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    output = model(sample_image)\n",
        "    probabilities = torch.softmax(output, dim=1)\n",
        "    predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "    print(f\"Predicted class index: {predicted_class_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# کلاس‌های CIFAR-10\n",
        "classes = (\n",
        "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# گرفتن یک batch از testloader\n",
        "images, labels = next(iter(testloader))\n",
        "\n",
        "# انتخاب یک تصویر (مثلاً اولی)\n",
        "image = images[102].unsqueeze(0).to(DEVICE)   # (1, 3, 32, 32)\n",
        "label = labels[102].item()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    probs = torch.softmax(outputs, dim=1)\n",
        "    pred_idx = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "print(f\"True label: {classes[label]}\")\n",
        "print(f\"Predicted label: {classes[pred_idx]}\")\n",
        "\n",
        "# برای نمایش تصویر باید نرمال‌سازی رو برگردونیم\n",
        "img = images[102].cpu()\n",
        "\n",
        "# اگر transform نرمال‌سازی داشته (مثلاً mean/std)\n",
        "# این قسمت رو مطابق transform خودت تنظیم کن\n",
        "img = img * 0.5 + 0.5  # مثال برای mean=0.5 std=0.5\n",
        "img = img.numpy().transpose((1, 2, 0))\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.title(f\"GT: {classes[label]} | Pred: {classes[pred_idx]}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "mH7WDTM6YF3E",
        "outputId": "b26f5622-4375-42e4-cf96-975fd79a2e86"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.7091274..1.6116571].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label: frog\n",
            "Predicted label: frog\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZveWV9TVInf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}